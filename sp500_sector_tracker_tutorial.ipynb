{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "976c0bf5",
   "metadata": {},
   "source": [
    "\n",
    "# Sector-Constrained Bayesian Index Tracker for the S&P 500 — A Hands-On Tutorial\n",
    "\n",
    "**Last updated:** 2025-08-09\n",
    "\n",
    "Welcome! This notebook walks you through building a **sector-constrained, Bayesian-regularized index tracker** that approximates the **S&P 500** using only the sectors you choose.\n",
    "\n",
    "You will:\n",
    "- Scrape **S&P 500 constituents + GICS sectors** from Wikipedia.\n",
    "- Choose which **sectors** to invest in (e.g., Information Technology, Health Care, Financials).\n",
    "- Fetch **historical prices** for your universe and the S&P 500 index (^GSPC).\n",
    "- Estimate a **Bayesian ridge** (MAP) regularization strength from data.\n",
    "- Optionally **limit the number of names** (cardinality) using a greedy forward-selection heuristic.\n",
    "- Solve a **convex optimization** (quadratic program) to minimize tracking error with:\n",
    "  - **Shrinkage** (L2, Bayesian MAP),\n",
    "  - A **sector mix penalty** that nudges your portfolio toward a target sector distribution,\n",
    "  - **Long-only**, **sum of weights = 1**, and **per-name caps** constraints.\n",
    "- Evaluate **6-month out-of-sample (OOS)** tracking and build a simple **6‑month forecast**.\n",
    "\n",
    "> **Note:** This is for research and education. Not investment advice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32af2882",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Requirements\n",
    "\n",
    "Run the cell below to install (or confirm) the required packages.\n",
    "\n",
    "- `pandas`, `numpy`: data wrangling\n",
    "- `yfinance`: prices and market caps (Yahoo Finance)\n",
    "- `cvxpy`, `osqp`, `scs`: convex optimization (QP solver)\n",
    "- `scikit-learn`: `BayesianRidge` for evidence-based regularization\n",
    "- `requests`, `lxml`: Wikipedia scraping (via `pandas.read_html`)\n",
    "- `matplotlib`: charts (we will not set explicit colors)\n",
    "- `statsmodels`: simple AR(1) forecast\n",
    "- `tqdm`: progress bars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc7f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running on a fresh environment, uncomment this cell and run it once.\n",
    "# !pip install -q pandas numpy yfinance cvxpy osqp scikit-learn lxml requests tqdm matplotlib statsmodels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52f07f9",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Imports and global configuration\n",
    "\n",
    "We also set a **lookback** window and an **out-of-sample** (OOS) window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20056c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import warnings\n",
    "import datetime as dt\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import yfinance as yf\n",
    "import cvxpy as cp\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---- Reproducibility-ish (does not fix all randomness across solvers) ----\n",
    "np.random.seed(42)\n",
    "\n",
    "# ---- Config ----\n",
    "LOOKBACK_YEARS = 3       # how much history to fit on\n",
    "OOS_MONTHS     = 6       # evaluate the next ~6 months of data\n",
    "FREQUENCY      = \"1d\"    # daily data\n",
    "INDEX_SYMBOL   = \"^GSPC\" # S&P 500 (Yahoo Finance symbol)\n",
    "UNIVERSE_CAP   = 300     # keep top-N by market cap in your selected sectors\n",
    "MAX_NAMES      = 40      # limit the number of names via greedy selection\n",
    "PER_NAME_CAP   = 0.08    # max weight per stock (e.g., 8%)\n",
    "LONG_ONLY      = True    # long-only or allow shorting\n",
    "SECTOR_PENALTY = 5.0     # gamma: strength of sector-mix penalty (0 = off)\n",
    "L2_FLOOR       = 1e-6    # minimal ridge regularization\n",
    "OUTPUT_DIR     = \"./notebook_outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Choose your sectors (GICS names)\n",
    "ALLOWED_SECTORS = [\n",
    "    \"Information Technology\",\n",
    "    \"Health Care\",\n",
    "    \"Financials\",\n",
    "    # You can add more:\n",
    "    # \"Communication Services\",\"Consumer Discretionary\",\"Consumer Staples\",\n",
    "    # \"Energy\",\"Industrials\",\"Materials\",\"Real Estate\",\"Utilities\"\n",
    "]\n",
    "\n",
    "# Date range for downloads\n",
    "today = dt.date.today()\n",
    "END_DATE = today\n",
    "START_DATE = END_DATE - dt.timedelta(days=int(365 * LOOKBACK_YEARS + 365))  # +1y buffer\n",
    "print(f\"Training window start: {START_DATE}, end: {END_DATE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d54ec5",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Get the S&P 500 constituents and sectors (Wikipedia)\n",
    "\n",
    "We use `pandas.read_html` to read the constituents table and then normalize tickers for Yahoo Finance (e.g., `BRK.B → BRK-B`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a1ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WIKI_URL = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "\n",
    "def get_sp500_constituents() -> pd.DataFrame:\n",
    "    \"\"\"Return a dataframe with columns: Ticker, Name, Sector, SubIndustry.\"\"\"\n",
    "    html = requests.get(WIKI_URL, timeout=30).text\n",
    "    tables = pd.read_html(html)\n",
    "    df = tables[0].rename(columns={\n",
    "        \"Symbol\": \"Ticker\",\n",
    "        \"Security\": \"Name\",\n",
    "        \"GICS Sector\": \"Sector\",\n",
    "        \"GICS Sub-Industry\": \"SubIndustry\"\n",
    "    })\n",
    "    # Normalize Yahoo tickers (e.g., BRK.B -> BRK-B)\n",
    "    df[\"Ticker\"] = df[\"Ticker\"].str.replace(\".\", \"-\", regex=False).str.upper().str.strip()\n",
    "    return df[[\"Ticker\", \"Name\", \"Sector\", \"SubIndustry\"]]\n",
    "\n",
    "sp500_all = get_sp500_constituents()\n",
    "sp500_all.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74605bd3",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Filter by your chosen sectors\n",
    "\n",
    "We will invest only in the sectors you selected above in `ALLOWED_SECTORS`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3cdeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sp500 = sp500_all[sp500_all[\"Sector\"].isin(ALLOWED_SECTORS)].copy()\n",
    "if sp500.empty:\n",
    "    raise ValueError(\"No tickers found in the selected sectors. Please adjust ALLOWED_SECTORS.\")\n",
    "print(f\"Selected sectors: {sorted(set(sp500['Sector']))}\")\n",
    "print(f\"Tickers in selected sectors: {len(sp500)}\")\n",
    "sp500.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfaa919",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Fetch market caps and compute sector targets\n",
    "\n",
    "We approximate sector weights by summing live **market caps** for each sector and normalizing them to 1.  \n",
    "This is a proxy for float-adjusted market-cap weights (it is not exact).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fc8401",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_market_caps(tickers: List[str]) -> pd.Series:\n",
    "    \"\"\"Fetch (approximate) market caps for tickers using yfinance.\n",
    "    Returns a Series indexed by ticker with values as floats.\n",
    "    \"\"\"\n",
    "    caps = {}\n",
    "    for t in tqdm(tickers, desc=\"Fetching market caps\"):\n",
    "        try:\n",
    "            tk = yf.Ticker(t)\n",
    "            cap = None\n",
    "            # Try fast_info first\n",
    "            try:\n",
    "                cap = tk.fast_info.get(\"market_cap\", None)\n",
    "            except Exception:\n",
    "                pass\n",
    "            if cap is None:\n",
    "                info = tk.info\n",
    "                cap = info.get(\"marketCap\", None)\n",
    "            if cap is not None and np.isfinite(cap):\n",
    "                caps[t] = float(cap)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return pd.Series(caps, name=\"MarketCap\", dtype=\"float64\")\n",
    "\n",
    "# Compute sector targets using ALL S&P 500 (more realistic), or only selected sectors.\n",
    "# Here we use the full list to estimate the reference mix:\n",
    "caps_reference = fetch_market_caps(sp500_all[\"Ticker\"].tolist())\n",
    "ref_df = sp500_all.merge(caps_reference.rename(\"MarketCap\"),\n",
    "                         left_on=\"Ticker\", right_index=True, how=\"left\").dropna(subset=[\"MarketCap\"])\n",
    "sector_caps_ref = ref_df.groupby(\"Sector\")[\"MarketCap\"].sum()\n",
    "sector_target = (sector_caps_ref / sector_caps_ref.sum()).sort_values(ascending=False)\n",
    "\n",
    "# Now attach caps to our selected-universe tickers\n",
    "caps_selected = fetch_market_caps(sp500[\"Ticker\"].tolist())\n",
    "sp500 = sp500.merge(caps_selected.rename(\"MarketCap\"),\n",
    "                    left_on=\"Ticker\", right_index=True, how=\"left\").dropna(subset=[\"MarketCap\"])\n",
    "\n",
    "print(\"Reference sector target (top 5 shown):\")\n",
    "sector_target.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01963994",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Cap the investable universe by size (optional)\n",
    "\n",
    "To keep downloads and optimization fast, we restrict to the top **UNIVERSE_CAP** names by market cap within your chosen sectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e7c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sp500 = sp500.sort_values(\"MarketCap\", ascending=False).head(UNIVERSE_CAP).reset_index(drop=True)\n",
    "print(f\"Universe size after capping: {len(sp500)}\")\n",
    "sp500.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8ee4db",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Download historical prices\n",
    "\n",
    "We download **adjusted close** prices for the S&P 500 index (`^GSPC`) and for our universe.  \n",
    "We then compute **daily log returns** and split into **in-sample** and **out-of-sample** windows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea64706",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_prices(tickers: List[str], start, end, interval=\"1d\") -> pd.DataFrame:\n",
    "    \"\"\"Batch download adjusted close prices via yfinance.\n",
    "    Returns a DataFrame with dates as index and tickers as columns.\n",
    "    \"\"\"\n",
    "    data = yf.download(tickers, start=start, end=end, interval=interval,\n",
    "                       auto_adjust=True, progress=False)\n",
    "    px = data[\"Close\"] if \"Close\" in data else data\n",
    "    if isinstance(px, pd.Series):\n",
    "        px = px.to_frame(tickers[0])\n",
    "    return px.dropna(how=\"all\")\n",
    "\n",
    "tickers = sp500[\"Ticker\"].tolist()\n",
    "px_assets = fetch_prices(tickers, START_DATE, END_DATE, FREQUENCY)\n",
    "px_index  = fetch_prices([INDEX_SYMBOL], START_DATE, END_DATE, FREQUENCY)\n",
    "\n",
    "# Align dates and drop columns with missing data\n",
    "common_idx = px_assets.index.intersection(px_index.index)\n",
    "px_assets = px_assets.loc[common_idx].dropna(axis=1, how=\"any\")\n",
    "px_index  = px_index.loc[common_idx]\n",
    "\n",
    "# Align universe to available data\n",
    "available = [t for t in tickers if t in px_assets.columns]\n",
    "sp500 = sp500[sp500[\"Ticker\"].isin(available)].reset_index(drop=True)\n",
    "tickers = sp500[\"Ticker\"].tolist()\n",
    "\n",
    "def log_returns(px: pd.DataFrame) -> pd.DataFrame:\n",
    "    return np.log(px).diff().dropna()\n",
    "\n",
    "R = log_returns(px_assets)       # T x N asset log-returns\n",
    "r_idx = log_returns(px_index)    # T x 1 index log-returns\n",
    "r_idx = r_idx.iloc[:, 0]         # convert to Series\n",
    "\n",
    "# Split IS/OOS\n",
    "oos_days = int(30.42 * OOS_MONTHS)  # ~ trading days metric\n",
    "if len(R) < oos_days + 252:\n",
    "    raise ValueError(\"Not enough history for requested OOS. Increase LOOKBACK_YEARS or reduce OOS_MONTHS.\")\n",
    "\n",
    "R_is   = R.iloc[:-oos_days, :]\n",
    "R_oos  = R.iloc[-oos_days:, :]\n",
    "y_is   = r_idx.iloc[:-oos_days]\n",
    "y_oos  = r_idx.iloc[-oos_days:]\n",
    "R_is.shape, R_oos.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bdfa6d",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Build the sector aggregation matrix\n",
    "\n",
    "We need a matrix **A** with shape `(num_sectors, num_stocks)` where `A[s, j] = 1` if stock `j` is in sector `s`, else 0.  \n",
    "This allows us to penalize deviations of the **portfolio sector weights** from the **target sector mix**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ed9efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sectors present in our (post-cleaning) universe\n",
    "sectors = sorted(sp500[\"Sector\"].unique())\n",
    "S = len(sectors)\n",
    "sector_to_row = {s: i for i, s in enumerate(sectors)}\n",
    "\n",
    "# Build A for the FULL universe (we will rebuild it again after selection)\n",
    "N_full = len(tickers)\n",
    "A_full = np.zeros((S, N_full), dtype=float)\n",
    "for j, t in enumerate(tickers):\n",
    "    s = sp500.loc[sp500[\"Ticker\"] == t, \"Sector\"].values[0]\n",
    "    A_full[sector_to_row[s], j] = 1.0\n",
    "\n",
    "# Target vector aligned to `sectors`\n",
    "s_target = np.array([sector_target.get(s, 0.0) for s in sectors], dtype=float)\n",
    "if s_target.sum() > 0:\n",
    "    s_target = s_target / s_target.sum()\n",
    "\n",
    "S, N_full, s_target[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c096836c",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Estimate Bayesian ridge regularization (λ) via evidence maximization\n",
    "\n",
    "We fit `BayesianRidge` on **standardized** in-sample returns to obtain an effective L2 penalty.  \n",
    "This gives a **MAP** solution equivalent to ridge regression, which stabilizes the portfolio weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819602e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_lambda_bayesian(R: pd.DataFrame, y: pd.Series, l2_floor: float = 1e-6) -> float:\n",
    "    X = R.values\n",
    "    Y = y.values\n",
    "    # Standardize for evidence stability (the QP will use unstandardized returns)\n",
    "    Xs = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-12)\n",
    "    Ys = (Y - Y.mean()) / (Y.std() + 1e-12)\n",
    "    br = BayesianRidge(compute_score=True, fit_intercept=True)\n",
    "    br.fit(Xs, Ys)\n",
    "    lam = float(br.lambda_ / max(br.alpha_, 1e-12))\n",
    "    return max(lam, l2_floor)\n",
    "\n",
    "ridge_lambda = estimate_lambda_bayesian(R_is, y_is, L2_FLOOR)\n",
    "print(f\"Estimated ridge lambda (Bayesian evidence): {ridge_lambda:.4g}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9e24fc",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Greedy forward selection (optional) to limit the number of names\n",
    "\n",
    "Exact cardinality constraints make the problem **mixed-integer** and slow.  \n",
    "Instead, we use a **greedy** heuristic: iteratively add the stock most correlated with the **current residual** vs the index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61461b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def greedy_select(R: pd.DataFrame, y: pd.Series, lam: float, K: int | None) -> List[str]:\n",
    "    if K is None or K >= R.shape[1]:\n",
    "        return list(R.columns)\n",
    "\n",
    "    chosen = []\n",
    "    remaining = set(R.columns)\n",
    "\n",
    "    # Start with the max |corr| to index\n",
    "    corr = R.corrwith(y).abs().sort_values(ascending=False)\n",
    "    first = corr.index[0]\n",
    "    chosen.append(first)\n",
    "    remaining.remove(first)\n",
    "\n",
    "    for _ in range(1, K):\n",
    "        # Fit ridge on current chosen to get residuals\n",
    "        Xc = R[chosen].values\n",
    "        yc = y.values\n",
    "        XtX = Xc.T @ Xc\n",
    "        w = np.linalg.solve(XtX + lam*np.eye(len(chosen)), Xc.T @ yc)\n",
    "        resid = yc - Xc @ w\n",
    "\n",
    "        # Pick remaining name with highest |corr| to residual\n",
    "        best_t, best_score = None, -1.0\n",
    "        for t in remaining:\n",
    "            x = R[t].values\n",
    "            num = np.dot(resid - resid.mean(), x - x.mean())\n",
    "            den = np.sqrt(((resid - resid.mean())**2).sum() * ((x - x.mean())**2).sum()) + 1e-18\n",
    "            c = abs(num) / den\n",
    "            if c > best_score:\n",
    "                best_score, best_t = c, t\n",
    "\n",
    "        chosen.append(best_t)\n",
    "        remaining.remove(best_t)\n",
    "\n",
    "    return chosen\n",
    "\n",
    "selected_tickers = greedy_select(R_is, y_is, ridge_lambda, MAX_NAMES)\n",
    "len(selected_tickers), selected_tickers[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b07a9de",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Rebuild the sector matrix for the selected set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a07ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "R_is_sel  = R_is[selected_tickers]\n",
    "R_oos_sel = R_oos[selected_tickers]\n",
    "sp500_sel = sp500[sp500[\"Ticker\"].isin(selected_tickers)].reset_index(drop=True)\n",
    "\n",
    "# Rebuild A for the selected universe\n",
    "sectors_sel = sorted(sp500_sel[\"Sector\"].unique())\n",
    "row_sel = {s: i for i, s in enumerate(sectors_sel)}\n",
    "S_sel = len(sectors_sel)\n",
    "N_sel = len(selected_tickers)\n",
    "\n",
    "A_sel = np.zeros((S_sel, N_sel), dtype=float)\n",
    "for j, t in enumerate(selected_tickers):\n",
    "    s = sp500_sel.loc[sp500_sel[\"Ticker\"] == t, \"Sector\"].values[0]\n",
    "    A_sel[row_sel[s], j] = 1.0\n",
    "\n",
    "# Align target vector to sectors_sel\n",
    "s_tgt_sel = np.array([sector_target.get(s, 0.0) for s in sectors_sel], dtype=float)\n",
    "if s_tgt_sel.sum() > 0:\n",
    "    s_tgt_sel = s_tgt_sel / s_tgt_sel.sum()\n",
    "\n",
    "S_sel, N_sel, sectors_sel[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6cb3ac",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Solve the sector-constrained Bayesian ridge QP\n",
    "\n",
    "We minimize:\n",
    "\\[\n",
    "\\frac{1}{T}\\lVert Rw - y \\rVert^2 \\; + \\; \\lambda \\lVert w \\rVert^2 \\; + \\; \\gamma \\lVert A w - s^* \\rVert^2\n",
    "\\]\n",
    "Subject to:\n",
    "- \\( \\sum_i w_i = 1 \\)\n",
    "- Long-only (optional): \\( w_i \\ge 0 \\)\n",
    "- Per-name cap (optional): \\( w_i \\le c \\)\n",
    "\n",
    "We use **cvxpy** and the **OSQP** solver.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2746eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def solve_tracker_qp(Rm: np.ndarray,\n",
    "                     y: np.ndarray,\n",
    "                     A: np.ndarray,\n",
    "                     s_target: np.ndarray,\n",
    "                     ridge_lambda: float,\n",
    "                     gamma: float,\n",
    "                     long_only: bool,\n",
    "                     w_cap: float | None) -> np.ndarray:\n",
    "    T, N = Rm.shape\n",
    "    w = cp.Variable(N)\n",
    "\n",
    "    # Tracking error\n",
    "    resid = Rm @ w - y\n",
    "    loss_track = (1.0 / T) * cp.sum_squares(resid)\n",
    "\n",
    "    # Ridge regularization\n",
    "    loss_ridge = ridge_lambda * cp.sum_squares(w)\n",
    "\n",
    "    # Sector penalty\n",
    "    loss_sector = 0\n",
    "    if gamma > 0 and s_target.sum() > 0:\n",
    "        loss_sector = gamma * cp.sum_squares(A @ w - s_target)\n",
    "\n",
    "    objective = cp.Minimize(loss_track + loss_ridge + loss_sector)\n",
    "\n",
    "    constraints = [cp.sum(w) == 1.0]\n",
    "    if long_only:\n",
    "        constraints += [w >= 0.0]\n",
    "        if w_cap is not None:\n",
    "            constraints += [w <= w_cap]\n",
    "\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    try:\n",
    "        prob.solve(solver=cp.OSQP, eps_abs=1e-7, eps_rel=1e-7, verbose=False)\n",
    "    except Exception:\n",
    "        # Fallback solver\n",
    "        prob.solve(solver=cp.SCS, verbose=False)\n",
    "\n",
    "    if w.value is None:\n",
    "        raise RuntimeError(\"Optimization failed. Try relaxing constraints or parameters.\")\n",
    "    return np.array(w.value).ravel()\n",
    "\n",
    "weights = solve_tracker_qp(\n",
    "    Rm=R_is_sel.values,\n",
    "    y=y_is.values,\n",
    "    A=A_sel,\n",
    "    s_target=s_tgt_sel,\n",
    "    ridge_lambda=ridge_lambda,\n",
    "    gamma=SECTOR_PENALTY,\n",
    "    long_only=LONG_ONLY,\n",
    "    w_cap=PER_NAME_CAP if LONG_ONLY else None\n",
    ")\n",
    "\n",
    "weights_series = pd.Series(weights, index=R_is_sel.columns, name=\"Weight\").sort_values(ascending=False)\n",
    "weights_series.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50c73a6",
   "metadata": {},
   "source": [
    "\n",
    "## 12. Evaluate in-sample (IS) and out-of-sample (OOS) tracking\n",
    "\n",
    "We compute portfolio returns, tracking error (TE), annualized TE volatility, beta, and R².\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bd42f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_tracking(R: pd.DataFrame, y: pd.Series, w: pd.Series) -> Tuple[pd.Series, Dict[str, float]]:\n",
    "    rp = (R[w.index] @ w.values).rename(\"Portfolio\")\n",
    "    te = rp - y\n",
    "    TE_vol = float(te.std() * np.sqrt(252))\n",
    "    TE_mean = float(te.mean() * 252)\n",
    "    R2 = float(np.corrcoef(rp, y)[0, 1]**2) if len(rp) > 2 else np.nan\n",
    "    beta = float(np.cov(rp, y, ddof=1)[0, 1] / (rp.var(ddof=1) + 1e-12))\n",
    "    return rp, {\n",
    "        \"TE_ann_vol\": TE_vol,\n",
    "        \"TE_ann_mean\": TE_mean,\n",
    "        \"R2\": R2,\n",
    "        \"Beta\": beta,\n",
    "        \"Names\": int((w.values > 1e-6).sum())\n",
    "    }\n",
    "\n",
    "rp_is, met_is   = evaluate_tracking(R_is_sel, y_is, weights_series)\n",
    "rp_oos, met_oos = evaluate_tracking(R_oos_sel, y_oos, weights_series)\n",
    "\n",
    "print(\"In-sample metrics:\", json.dumps(met_is, indent=2))\n",
    "print(\"OOS metrics:\", json.dumps(met_oos, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7e62e1",
   "metadata": {},
   "source": [
    "\n",
    "## 13. Plot cumulative growth (IS and OOS)\n",
    "\n",
    "We **do not** set explicit colors, and we keep each chart as a **single plot**, as a clean best practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2950b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In-sample cumulative growth\n",
    "cum_is = pd.concat([rp_is.rename(\"Portfolio\"), y_is.rename(\"Index\")], axis=1).cumsum().apply(np.exp)\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(cum_is.index, cum_is[\"Portfolio\"], label=\"Portfolio\")\n",
    "plt.plot(cum_is.index, cum_is[\"Index\"], label=\"Index\")\n",
    "plt.title(\"In-sample cumulative growth\")\n",
    "plt.ylabel(\"Growth of $1\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# OOS cumulative growth\n",
    "cum_oos = pd.concat([rp_oos.rename(\"Portfolio\"), y_oos.rename(\"Index\")], axis=1).cumsum().apply(np.exp)\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(cum_oos.index, cum_oos[\"Portfolio\"], label=\"Portfolio\")\n",
    "plt.plot(cum_oos.index, cum_oos[\"Index\"], label=\"Index\")\n",
    "plt.title(\"Out-of-sample (~6 months) cumulative growth\")\n",
    "plt.ylabel(\"Growth of $1\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d085171",
   "metadata": {},
   "source": [
    "\n",
    "## 14. Compare sector mix (Portfolio vs Target)\n",
    "\n",
    "We compute the portfolio sector weights as `A @ w` and compare to the target vector aligned to `sectors_sel`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e94046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "port_sector_weights = pd.Series(A_sel @ weights_series.loc[selected_tickers].values,\n",
    "                                index=sectors_sel, name=\"Portfolio\")\n",
    "tgt_sector_weights  = pd.Series([sector_target.get(s, 0.0) for s in sectors_sel],\n",
    "                                index=sectors_sel, name=\"Target\")\n",
    "if tgt_sector_weights.sum() > 0:\n",
    "    tgt_sector_weights = tgt_sector_weights / tgt_sector_weights.sum()\n",
    "\n",
    "sec_df = pd.concat([port_sector_weights, tgt_sector_weights], axis=1).fillna(0)\n",
    "\n",
    "# Bar chart (two separate calls keep a single figure)\n",
    "plt.figure(figsize=(10, 4))\n",
    "# Portfolio\n",
    "plt.bar(sec_df.index, sec_df[\"Portfolio\"])\n",
    "plt.title(\"Portfolio sector weights\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "# Target\n",
    "plt.bar(sec_df.index, sec_df[\"Target\"])\n",
    "plt.title(\"Target sector weights\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "sec_df.sort_values(\"Target\", ascending=False).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06ca113",
   "metadata": {},
   "source": [
    "\n",
    "## 15. Simple 6-month forecast (AR(1) on index returns mapped by beta)\n",
    "\n",
    "This **toy** forecast fits an AR(1) to **in-sample index returns**, then maps to the portfolio by multiplying with the **in-sample beta**.  \n",
    "Replace with Black-Litterman, block bootstrap, or a macro model for more realism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e36192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "# beta from in-sample\n",
    "beta_is = met_is[\"Beta\"]\n",
    "\n",
    "# Fit AR(1) on in-sample index returns\n",
    "ar = AutoReg(y_is.values, lags=1, old_names=False).fit()\n",
    "\n",
    "steps = len(y_oos)  # forecast horizon ≈ OOS window length\n",
    "y_fore = ar.predict(start=len(y_is), end=len(y_is) + steps - 1)\n",
    "p_fore = beta_is * y_fore\n",
    "\n",
    "df_fore = pd.DataFrame({\"Index_fore\": y_fore, \"Portfolio_fore\": p_fore})\n",
    "\n",
    "cum_fore = df_fore.cumsum().apply(np.exp)\n",
    "plt.figure(figsize=(9, 4))\n",
    "plt.plot(cum_fore.index, cum_fore[\"Portfolio_fore\"], label=\"Portfolio (forecast)\")\n",
    "plt.plot(cum_fore.index, cum_fore[\"Index_fore\"], label=\"Index (forecast)\")\n",
    "plt.title(\"Forecast (~6 months) cumulative growth\")\n",
    "plt.ylabel(\"Growth of $1\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df_fore.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd46746",
   "metadata": {},
   "source": [
    "\n",
    "## 16. Save weights and metrics\n",
    "\n",
    "We write weights and metrics to CSV/JSON for downstream use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aab63d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights_df = (weights_series.to_frame()\n",
    "              .merge(sp500_sel[[\"Ticker\",\"Name\",\"Sector\",\"MarketCap\"]],\n",
    "                     left_index=True, right_on=\"Ticker\")\n",
    "              .sort_values(\"Weight\", ascending=False))\n",
    "weights_path = os.path.join(OUTPUT_DIR, \"weights.csv\")\n",
    "weights_df.to_csv(weights_path, index=False)\n",
    "\n",
    "metrics = {\n",
    "    \"in_sample\": met_is,\n",
    "    \"oos\": met_oos,\n",
    "    \"lambda\": ridge_lambda,\n",
    "    \"gamma\": SECTOR_PENALTY,\n",
    "    \"long_only\": LONG_ONLY,\n",
    "    \"per_name_cap\": PER_NAME_CAP,\n",
    "    \"lookback_years\": LOOKBACK_YEARS,\n",
    "    \"oos_months\": OOS_MONTHS,\n",
    "    \"universe_cap\": UNIVERSE_CAP,\n",
    "    \"K\": MAX_NAMES,\n",
    "    \"sectors\": ALLOWED_SECTORS\n",
    "}\n",
    "metrics_path = os.path.join(OUTPUT_DIR, \"metrics.json\")\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "weights_path, metrics_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2799ded0",
   "metadata": {},
   "source": [
    "\n",
    "## 17. Extensions and next steps\n",
    "\n",
    "- **Exact cardinality**: use a mixed-integer solver with binary variables for on/off holdings (slower).\n",
    "- **Shorting**: allow negative weights and add an L1 (or gross exposure) constraint to control leverage.\n",
    "- **Different priors**: try elastic-net or hierarchical priors instead of ridge.\n",
    "- **Turnover control**: add a term penalizing \\(\\lVert w_t - w_{t-1} \\rVert\\) and simulate rebalancing.\n",
    "- **Better forecast**: plug in Black-Litterman, bootstrapped scenarios, VAR, or macro-driven models.\n",
    "- **Data refinements**: use float-adjusted market caps; handle corporate actions; align to true index rebalances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e41261",
   "metadata": {},
   "source": [
    "\n",
    "## 18. Environment information (versions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2271b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, platform\n",
    "import sklearn, cvxpy, statsmodels\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "print(\"scikit-learn:\", sklearn.__version__)\n",
    "print(\"cvxpy:\", cvxpy.__version__)\n",
    "print(\"statsmodels:\", statsmodels.__version__)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
